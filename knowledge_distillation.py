# -*- coding: utf-8 -*-
"""Knowledge Distillation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1Mh9IcivB2X0Ewkk2uFtexJA2QW5bLr
"""

!pip install transformers

from transformers import TrainingArguments

class KnowledgeDistillationTrainingArguments(TrainingArguments):
  def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):

    super().__init__(*args, **kwargs)

    self.alpha = alpha
    self.temperature = temperature

import torch.nn as nn
import torch.nn.functional as F
from transformers import Trainer

class KnowledgeDistillationTrainer(Trainer):
  def __init__(self, *args, teacher_model=None, **kwargs):
    super().__init__(*args, **kwargs)
    self.teacher_model = teacher_model

  def compute_loss(self, model, inputs, return_outputs=False,**kwargs):
    #Extract cross-entropy loss and logits from student
    outputs_student = model(**inputs)
    loss_ce = outputs_student.loss
    logits_student = outputs_student.logits

    #Extract logits from teacher
    outputs_teacher = self.teacher_model(**inputs)
    logits_teacher = outputs_teacher.logits


    #Computing distillation loss by Softening probabilities
    loss_fct = nn.KLDivLoss(reduction="batchmean")
    #The reduction=batchmean argument in nn.KLDivLoss() specifies that we average the losses over the batch dimension.
    loss_kd = self.args.temperature ** 2 * loss_fct(
                F.log_softmax(logits_student / self.args.temperature, dim=-1),
                F.softmax(logits_teacher / self.args.temperature, dim=-1))

    # Return weighted student loss
    loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd
    return (loss, outputs_student) if return_outputs else loss

!pip install datasets

from datasets import load_dataset

clinc = load_dataset("clinc_oos", "plus")

sample = clinc["train"]
sample1 = clinc["train"][0]
sample2 = clinc["train"]["intent"]
print(sample)
print(sample1)
print(sample2)

intents = clinc["train"].features["intent"]
intent = intents.int2str(sample2)
print(intent)

from transformers import AutoTokenizer

student_checkpoint = "distilbert-base-uncased"
student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)

def tokenize_text(batch):
  return student_tokenizer(batch["text"], truncation=True)

clinc_tokenized = clinc.map(tokenize_text, batched=True, remove_columns=["text"])

clinc_tokenized = clinc_tokenized.rename_column("intent", "labels")

pip install evaluate

import numpy as np
from evaluate import load
accuracy_score = load("accuracy")

def compute_metrics(pred):
  predictions, labels = pred
  predictions = np.argmax(predictions, axis=1)
  return accuracy_score.compute(predictions=predictions, references=labels)

batch_size = 48
finetuned_student_ckpt = "distilbert-base-uncased-finetuned-clinc-student"

!pip install accelerate

student_training_args = KnowledgeDistillationTrainingArguments(
    output_dir=finetuned_student_ckpt, eval_strategy = "epoch",
    num_train_epochs=1, learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size, alpha=0.7, weight_decay=0.01)

from transformers import pipeline

bert_ckpt = "transformersbook/bert-base-uncased-finetuned-clinc"
pipe = pipeline("text-classification", model=bert_ckpt)

id2label = pipe.model.config.id2label
label2id = pipe.model.config.label2id

from transformers import AutoConfig
num_labels = intents.num_classes
student_config = (AutoConfig
                  .from_pretrained(student_checkpoint, num_labels=num_labels,
                                    id2label=id2label, label2id=label2id))

import torch
from transformers import AutoModelForSequenceClassification
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def student_init():
  return (AutoModelForSequenceClassification.from_pretrained(student_checkpoint, config=student_config).to(device))

teacher_checkpoint = "transformersbook/bert-base-uncased-finetuned-clinc"

teacher_model = (AutoModelForSequenceClassification
                     .from_pretrained(teacher_checkpoint, num_labels=num_labels)
                     .to(device))

#Lets start the training
distilbert_trainer = KnowledgeDistillationTrainer(model_init=student_init,
        teacher_model=teacher_model, args=student_training_args,
        train_dataset=clinc_tokenized['train'], eval_dataset=clinc_tokenized['validation'],
        compute_metrics=compute_metrics, tokenizer=student_tokenizer)
distilbert_trainer.train()

def save_teacher_model():
  teacher_model.save_pretrained("teacher_model")
def save_student_model():
  distilbert_trainer.save_model('student_model')

save_teacher_model()
save_student_model()

from transformers import AutoConfig, AutoModelForSequenceClassification
import os

def compute_parameters(model_path):
  model = AutoModelForSequenceClassification.from_pretrained(model_path)
  parameters = model.num_parameters()
  return parameters

teacher_model_parameters = compute_parameters(model_path="/content/teacher_model")
print("Teacher Model: ", teacher_model_parameters)

student_model_parameters = compute_parameters(model_path="/content/student_model")
print("Student Model: ", student_model_parameters)

decrease = (student_model_parameters-teacher_model_parameters)/teacher_model_parameters
print(decrease*100)

!ls /content/student_model -al --block-size=MB

!ls /content/teacher_model -al --block-size=MB

from transformers import pipeline
import time

pipe = pipeline("text-classification", model="/content/teacher_model", tokenizer='bert-base-uncased')

sample_input = clinc['train']['text'][101]

#WARMUP
for _ in range(10):
  _ = pipe(sample_input)

start = time.time()
for _ in range(100):
  _ = pipe(sample_input)
total_time_teacher_model = time.time()-start
print("Total time to process 100 requests for Teacher Model: ",total_time_teacher_model)

pipe = pipeline("text-classification", model="/content/student_model", tokenizer="distilbert-base-uncased")

sample_input = clinc['train']['text'][101]

#WARMUP
for _ in range(10):
  _ = pipe(sample_input)

start = time.time()
for _ in range(100):
  _ = pipe(sample_input)
total_time_student_model = time.time()-start

print("Total time to process 100 requests for Student Model: ",total_time_student_model)

decrease_in_time = (total_time_teacher_model-total_time_student_model)/total_time_teacher_model
print(decrease_in_time*100)

# Student evaluation
student_results = distilbert_trainer.evaluate()

print("Student:", student_results)

import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=student_tokenizer)

def evaluate_model(model, dataloader, device):
    model.eval()
    preds, labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            preds.extend(np.argmax(logits.cpu().numpy(), axis=1))
            labels.extend(batch["labels"].cpu().numpy())

    return np.array(preds), np.array(labels)

# Dataloader for validation set
val_loader = DataLoader(
    clinc_tokenized["validation"],
    batch_size=32,
    collate_fn=data_collator
)

val_loader = DataLoader(
    clinc_tokenized["validation"],
    batch_size=32,
    collate_fn=data_collator
)

# Evaluate teacher and student
teacher_preds, labels = evaluate_model(teacher_model, val_loader, device)
student_preds, _      = evaluate_model(distilbert_trainer.model, val_loader, device)

# Compute metrics
def compute_all_metrics(preds, labels, model_name):
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
    print(f"\nðŸ“Š {model_name} Results:")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print("\nClassification Report:\n", classification_report(labels, preds))
    return acc, precision, recall, f1, confusion_matrix(labels, preds)

# Teacher metrics
teacher_acc, teacher_prec, teacher_rec, teacher_f1, teacher_cm = compute_all_metrics(teacher_preds, labels, "Teacher")

# Student metrics
student_acc, student_prec, student_rec, student_f1, student_cm = compute_all_metrics(student_preds, labels, "Student")

# Confusion Matrix Plot
def plot_confusion(cm, title):
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=False, cmap="Blues", fmt="d")
    plt.title(title)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

plot_confusion(teacher_cm, "Teacher Confusion Matrix")
plot_confusion(student_cm, "Student Confusion Matrix")

